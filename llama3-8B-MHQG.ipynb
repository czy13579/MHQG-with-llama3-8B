{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8078881,"sourceType":"datasetVersion","datasetId":4768049},{"sourceId":8375318,"sourceType":"datasetVersion","datasetId":4979755},{"sourceId":8740025,"sourceType":"datasetVersion","datasetId":5247299},{"sourceId":9145762,"sourceType":"datasetVersion","datasetId":5524156}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 数据准备","metadata":{}},{"cell_type":"code","source":"import json\nimport random\nfrom tqdm import tqdm\ndata_dict={}\ndef read_json(data_path,data_type):\n    if data_dict.get(data_type,-1)!=-1:\n        return data_dict[data_type]\n    else:\n        with open(data_path, 'r', encoding='utf-8') as f:\n            data =json.load(f)\n        data_dict[data_type]=data\n        return data\n    \ndef get_datas(data_path,data_type):\n    data=read_json(data_path,data_type)\n    answers=[]\n    questions=[]\n    title_to_contexts=[]\n    support_facts=[]\n    for i,d in tqdm(enumerate(data)):\n        ## 删除训练集中的单跳问题\n        if data_type=='train':\n            if d['level']=='easy':\n                continue\n        \n        ## 验证集只保留难度为hard的文段，结果较为稳定\n        if data_type=='dev':\n            if d['level']!='hard':\n                continue\n        \n        \n        answer=d['answer']\n        \n        ## 检查答案被包含在多少个文段中，如果答案出现的文段数多于两个，说明答案的质量不佳\n        paragraph_with_answer=0\n        for x in d['context']:\n            s=' '.join(x[1])\n            if s.find(answer)!=-1:\n                paragraph_with_answer+=1\n                \n        answer_token_num=len(answer.split())\n        ## 删除训练集中答案信息不充分的数据如yes,no\n        ## 答案中所有单词都为出现在文段中视为信息不充分\n        if paragraph_with_answer>2:\n            if data_type=='train':\n                p_drop=0.99\n                if random.random()<p_drop:\n                    continue\n                \n        if answer_token_num<=3 and paragraph_with_answer==0:\n            if data_type=='train':\n                p_drop=0.99\n                if random.random()<p_drop:\n                    continue\n        \n        if data_type=='train':\n            p_drop=1-0.2*answer_token_num\n            if random.random()<p_drop:\n                continue\n      \n        ## support_facts\n        support_facts_list=[]\n        fact_set=set()\n        if data_type=='train':\n            titles=[]\n            for item in d['supporting_facts']:\n                fact_set.add(item[0])\n            support_facts_list=list(fact_set)\n        else:\n            for item in d['pred_support_facts']:\n                support_facts_list.append(item[0])\n     \n            \n\n        ## title_to_context\n        title_to_context={}   \n        for x in d['context']:\n            s=' '.join(x[1])\n            context=f'{x[0]}. {s} '\n            #context=context.replace(f'{answer}',f'<ans> {answer} </ans>')\n            title_to_context[x[0]]=context\n\n        question=[]\n        if data_type!='test':\n            question=d['question']\n            \n        questions.append(question)\n        title_to_contexts.append(title_to_context)\n        answers.append(answer)\n        support_facts.append(support_facts_list)\n        \n\n    return title_to_contexts,questions,answers,support_facts","metadata":{"execution":{"iopub.status.busy":"2024-08-10T14:10:40.464443Z","iopub.execute_input":"2024-08-10T14:10:40.464845Z","iopub.status.idle":"2024-08-10T14:10:40.495790Z","shell.execute_reply.started":"2024-08-10T14:10:40.464811Z","shell.execute_reply":"2024-08-10T14:10:40.494816Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_context(title_to_context,support_facts_list,data_type='train',pg_num=10):\n    if data_type=='train':\n        if len(support_facts_list)>pg_num:\n                support_facts_list=support_facts_list[:pg_num]\n        else:\n            num=pg_num-len(support_facts_list)\n            titles=[]\n            for title in list(title_to_context.keys()):\n                if title not in support_facts_list:\n                    titles.append(title)\n            support_facts_list.extend(random.choices(titles,k=min(len(titles),num)))\n            random.shuffle(support_facts_list)\n    else:\n        support_facts_list=support_facts_list[:pg_num]\n        \n    context=''   \n    for title in support_facts_list:\n        context+=title_to_context[title]+'\\n'\n    return context\n\n## 封装成huggingface dataset\nimport datasets\ndef get_dataset(data_path,data_type,instruction,load=True):\n    ## 对于train_dataset,会进行保存，以保证每次加载数据是一致的，可以继续训练\n    if data_type=='train' and load :\n        try:\n            dataset=datasets.load_from_disk('/kaggle/working/llama')\n            return dataset\n        except:\n            pass\n    title_to_contexts,questions,answers,support_facts=get_datas(data_path,data_type)\n    inputs=[]\n    for i in range(len(answers)):\n        context=get_context(title_to_contexts[i],support_facts[i],data_type,pg_num=10)\n        input=f'Answer: {answers[i]} \\nContext:\\n{context}\\n'\n        inputs.append(input)\n    \n    dict_data={}\n    dict_data['instruction']=[instruction]*len(answers)\n    dict_data['input']=inputs\n    dict_data['output']=questions\n    if data_type!='train':\n        dict_data['output']=['']*len(answers)\n        dict_data['real_output']=questions\n    dataset=datasets.Dataset.from_dict(dict_data)\n    \n    if data_type=='train':\n        dataset.save_to_disk(dataset_path='./')\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-10T14:10:40.510418Z","iopub.execute_input":"2024-08-10T14:10:40.510712Z","iopub.status.idle":"2024-08-10T14:10:43.346446Z","shell.execute_reply.started":"2024-08-10T14:10:40.510687Z","shell.execute_reply":"2024-08-10T14:10:43.345625Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_path='/kaggle/input/scnu-ai-challenge-5/train.json'\ndev_path='/kaggle/input/scnu-ai-challenge-dataset-with-sorted-pred-facts/dev.json'\ntest_path='/kaggle/input/scnu-ai-challenge-dataset-with-sorted-pred-facts/test.json'\n## 设置指令\n##包含思维链\n## 先要求模型找出support_facts,再生成问题\ninstruction_with_Thinking_chain='The following is an answer and ten paragraphs. \\\nPlease think about which two paragraphs are most relevant to the given answer,\\\nthen generate a question according to the answer and two paragraphs.\\n\\\nPlease note that your Response needs to be in the following format.\\n\\\n## Question: \\n\\\n'\n## 不包含思维链\ninstruction1='Please generate only one question according to the answer and the paragraphs.'\n\ninstruction=instruction_with_Thinking_chain\ntrain_dataset=get_dataset(train_path,'train',instruction,load=True)\ndev_dataset=get_dataset(dev_path,'dev',instruction)\ntest_dataset=get_dataset(test_path,'test',instruction)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T14:10:43.347939Z","iopub.execute_input":"2024-08-10T14:10:43.348425Z","iopub.status.idle":"2024-08-10T14:10:47.131601Z","shell.execute_reply.started":"2024-08-10T14:10:43.348399Z","shell.execute_reply":"2024-08-10T14:10:47.130487Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"1500it [00:00, 81250.32it/s]\n7405it [00:00, 31894.29it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n ### Response:\n{}\"\"\"\n\nEOS_TOKEN = '<|end_of_text|>' # Must add EOS_TOKEN\n## 训练数据要加结束符，验证和测试不用\ntraining=True\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        if training:\n            output=f'## Question: {output}\\n{EOS_TOKEN}'\n        text = alpaca_prompt.format(instruction, input, output) #+ EOS_TOKEN\n        \n        texts.append(text)\n    return { \"text\" : texts, }\n\n\n\ntraining=True\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched = True,)\ntrain_dataset.shuffle(seed=42)\ntraining=False\ndev_dataset = dev_dataset.map(formatting_prompts_func, batched = True,)\ntest_dataset = test_dataset.map(formatting_prompts_func, batched = True,)\n\nprint('train_dataset:',len(train_dataset))\nprint('dev_dataset:',len(dev_dataset))\nprint('test_dataset:',len(test_dataset))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T14:10:47.133204Z","iopub.execute_input":"2024-08-10T14:10:47.134255Z","iopub.status.idle":"2024-08-10T14:10:47.748787Z","shell.execute_reply.started":"2024-08-10T14:10:47.134220Z","shell.execute_reply":"2024-08-10T14:10:47.747808Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/360 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d14cbde1d8e94689a7d363eb38eec809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004d27fd05ab41058568f8edfed7b38f"}},"metadata":{}},{"name":"stdout","text":"train_dataset: 25272\ndev_dataset: 360\ntest_dataset: 7405\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dev_dataset.select([1])[0]['text'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T14:10:47.762949Z","iopub.execute_input":"2024-08-10T14:10:47.763259Z","iopub.status.idle":"2024-08-10T14:10:47.775017Z","shell.execute_reply.started":"2024-08-10T14:10:47.763228Z","shell.execute_reply":"2024-08-10T14:10:47.773990Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nThe following is an answer and ten paragraphs. Please think about which two paragraphs are most relevant to the given answer,then generate a question according to the answer and two paragraphs.\nPlease note that your Response needs to be in the following format.\n## Question: \n\n\n### Input:\nAnswer: the tenth season \nContext:\nBigg Boss 10. Bigg Boss 10 is the tenth season of the Indian reality TV series \"Bigg Boss\".  It began airing on 16 October 2016 on Colors.  The show is also available after the original telecast on Viacom 18's digital platform – Voot.  A new element called ‘Unseen-Undekha’ was introduced by way of unseen footage uploaded on Voot. This footage showed parts of the day that weren’t included in the episode, from ‘wake-up call’ to ‘lights out’. \nLopamudra Raut. Lopamudra Raut is an Indian model and beauty queen from the state of Maharashtra.  She represented India at Miss United Continents 2016 pageant and was crowned 2nd runner up.  She also won the third \"Best National Costume\" award for India.  Previous representatives of India, Gail Nicole Da Silva in 2014 and Sushrii Shreya Mishraa in 2015 also won the award.  She was a contestant of Bigg Boss 10. \nBigg Boss 11. Bigg Boss 11 is the eleventh season of Indian reality TV series \"Bigg Boss\" that will be premiered on Colors TV.  Salman Khan will host this season for the seventh time in \"Bigg Boss\" history and third time in a row.  It is scheduled to premiere on 1 October 2017 Mon - Fri 10.30pm and SAT - SUN lun with the finale set in 2018 \nBigg Boss 9. Bigg Boss 9, also known as Bigg Boss: Double Trouble, (stylized as Bigg Boss: Nau), was the ninth season of the Indian reality TV series \"Bigg Boss\" that premiered on 11 October 2015 on Colors TV.  Salman Khan returned to host the ninth season. \nBigg Boss 1. Bigg Boss in 2006 was the first season of the Indian reality TV programme \"Bigg Boss\".  It aired on Sony Entertainment Television from 3 November 2006 to 26 January 2007, a total of 86 days.  Unlike other versions of \"Big Brother\", the Indian version uses celebrities as housemates, not members of the general public.  It was hosted by the Bollywood Actor \"Arshad Warsi\". \nBigg Boss 3. Bigg Boss 3 in 2009 was the third season of the Indian reality TV programme \"Bigg Boss\".  It began airing on 4 October 2009 on Colors with Amitabh Bachchan as the host and aired for 84 days concluding on 26 December 2009.  Vindu Dara Singh won the show while Pravesh Rana was declared the first runner-up and Poonam Dhillon was declared the second runner-up.  Vindu was awarded with a prize money of INR 10 million.  He was also announced the most stylish and bold contestant and won a Chevrolet Cruze.  This season, the house was located the city of Lonavla in the Indian state of Maharashtra. \nBigg Boss 4. Bigg Boss 4 in 2010 was the fourth season of Indian reality TV show \"Bigg Boss\", which aired on Colors from 3 October 2010.  This season was longer than its predecessor, \"Bigg Boss 3\" and lasted for 14 weeks (96 days) ending on 8 January 2011.  The show was hosted by Salman Khan. \nBigg Boss 2. Bigg Boss 2 was the 2008 second season of the Indian reality TV programme \"Bigg Boss\".  It began airing on 21 August 2008 on Colors.  Shilpa Shetty replaced Arshad Warsi as host of the show.  Fourteen handpicked housemates entered during the launch and were described \"newsmakers\" rather than celebrities, though the majority of the contestants were associated with Bollywood or Indian TV channels and other realities shows.  The housemates, considered strangers for each other, spent 98 days or nearly 14 weeks locked out together under one roof under the 24×7 supervision of 32 cameras fitted around the \"Bigg Boss\" house at Lonavala, a hill station about 100 km east of Mumbai. \nBigg Boss 6. Bigg Boss 6 was the sixth season of the Indian reality TV show Bigg Boss, which is telecast on the TV channel Colors.  \"Bigg Boss\" is the Indian edition of \"Big Brother\" TV series.  The season started from 7 October 2012.  Salman Khan, who was the host of the previous two seasons, returned as the host for the show.  The sixth season was launched as a \"Parivarik\" season with a Gujarati tagline- \"Alag che!\"  (English: It's different).  The producers claimed that the contestants on \"Bigg Boss 6\" will be presented with a cleaner, more \"family like image\".  The prize money was reduced to million () with an amount of 500,000s awarded to the \"most entertaining\" housemate each week from week 6 onwards.  The award was discontinued after four weeks for unknown reasons. \nBigg Boss 7. Bigg Boss 7 (tagline: \"Jannat Ka Wow Aur Jahannam Ka Aaw Dekhege Saath Saath\") is the seventh season of the Indian reality TV series \"Bigg Boss\" which aired on TV channel Colors TV from 15 September 2013, with Salman Khan returning as the host for the fourth time and this season is longer than its predecessor, \"Bigg Boss 6\" and lasted for 15 weeks (104 days) concluding on Saturday, 28 December 2013.  The seventh season was launched with the tagline- 'Jannat Ka Wow Aur Jahannam Ka Aaw Dekhege Saath Saath'.  The show started airing at 9:00 everyday from 15 September. \n\n\n\n ### Response:\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 加载模型","metadata":{}},{"cell_type":"code","source":"%%time\n!mamba install --force-reinstall aiohttp -y\n!pip install -U \"xformers<0.0.26\" --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n!pip install deepspeed\n!pip install -q --upgrade transformers deepspeed\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T14:10:47.776276Z","iopub.execute_input":"2024-08-10T14:10:47.776666Z","iopub.status.idle":"2024-08-10T14:16:47.498386Z","shell.execute_reply.started":"2024-08-10T14:10:47.776632Z","shell.execute_reply":"2024-08-10T14:16:47.496943Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nLooking for: ['aiohttp']\n\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nrapidsai/linux-64 (check zst) \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\nrapidsai/linux-64 (check zst) \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\nrapidsai/linux-64 (check zst) \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\nrapidsai/linux-64 (check zst) \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\nrapidsai/linux-64 (check zst) \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/linux-64 (check zst)                       Checked  0.5s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nrapidsai/noarch (check zst) \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\nrapidsai/noarch (check zst) \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\nrapidsai/noarch (check zst) \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/noarch (check zst)                         Checked  0.3s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/linux-64 (check zst)                        Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nnvidia/noarch (check zst) \u001b[33m━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/noarch (check zst)                           Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64 (check zst)                     Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\npkgs/main/noarch (check zst) \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch (check zst)                        Checked  0.0s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\npkgs/r/linux-64 (check zst) \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/linux-64 (check zst)                         Checked  0.0s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\npkgs/r/noarch (check zst) \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch (check zst)                           Checked  0.0s\n\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\nrapidsai/linux-64 \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nconda-forge/linux-64 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m  14.2kB /  36.8MB @ 155.1kB/s  0.1s\nrapidsai/linux-64    \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nrapidsai/noarch      \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nnvidia/linux-64      \u001b[90m━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nnvidia/noarch        \u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/noarch                                       12.7kB @ 127.0kB/s  0.1s\nnvidia/linux-64                                    203.1kB @   1.5MB/s  0.1s\nrapidsai/noarch                                     12.9kB @  68.2kB/s  0.2s\n[+] 0.2s\nconda-forge/linux-64 ━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4.2MB /  36.8MB @  21.7MB/s  0.2s\nconda-forge/noarch   \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\nrapidsai/linux-64    \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\npkgs/r/linux-64      \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\npkgs/r/noarch        \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\nconda-forge/linux-64 ━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m   6.6MB /  36.8MB @  26.5MB/s  0.3s\nconda-forge/noarch   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 123.7kB /  16.0MB @ 420.1kB/s  0.1s\nrapidsai/linux-64    ━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m 125.4kB / 361.8kB @ 489.4kB/s  0.3s\npkgs/r/linux-64      ━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━\u001b[0m   1.1MB /   1.6MB @   3.8MB/s  0.2s\npkgs/r/noarch        ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m   1.2MB /   2.1MB @   4.6MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/linux-64                                  361.8kB @   1.2MB/s  0.3s\npkgs/r/noarch                                        2.1MB @   6.4MB/s  0.3s\npkgs/r/linux-64                                      1.6MB @   4.5MB/s  0.3s\n[+] 0.4s\nconda-forge/linux-64 ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   7.8MB /  36.8MB @  22.0MB/s  0.4s\nconda-forge/noarch   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 426.0kB /  16.0MB @   1.2MB/s  0.2s\npkgs/main/linux-64   \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\npkgs/main/noarch     ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m 358.7kB / 715.6kB @ 904.0kB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                   715.6kB @   1.7MB/s  0.1s\n[+] 0.5s\nconda-forge/linux-64 ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m   9.9MB /  36.8MB @  21.4MB/s  0.5s\nconda-forge/noarch   ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1.3MB /  16.0MB @   2.8MB/s  0.3s\npkgs/main/linux-64   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 244.7kB /   6.3MB @ 527.1kB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\nconda-forge/linux-64 ━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━\u001b[0m  14.2MB /  36.8MB @  24.9MB/s  0.6s\nconda-forge/noarch   ━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m   2.3MB /  16.0MB @   4.1MB/s  0.4s\npkgs/main/linux-64   ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   1.2MB /   6.3MB @   2.1MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\nconda-forge/linux-64 ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m  18.7MB /  36.8MB @  27.7MB/s  0.7s\nconda-forge/noarch   ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m   3.8MB /  16.0MB @   5.6MB/s  0.5s\npkgs/main/linux-64   ━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   2.4MB /   6.3MB @   3.6MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\nconda-forge/linux-64 ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m  21.4MB /  36.8MB @  27.5MB/s  0.8s\nconda-forge/noarch   ━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   6.2MB /  16.0MB @   7.9MB/s  0.6s\npkgs/main/linux-64   ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m   3.8MB /   6.3MB @   4.9MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\nconda-forge/linux-64 ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m  24.4MB /  36.8MB @  27.6MB/s  0.9s\nconda-forge/noarch   ━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━\u001b[0m   9.1MB /  16.0MB @  10.2MB/s  0.7s\npkgs/main/linux-64   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m   4.9MB /   6.3MB @   5.5MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━\u001b[0m  27.2MB /  36.8MB @  27.6MB/s  1.0s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m  11.9MB /  16.0MB @  12.1MB/s  0.8s\npkgs/main/linux-64   ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m   5.8MB /   6.3MB @   5.8MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m  28.6MB /  36.8MB @  27.6MB/s  1.1s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m  13.4MB /  16.0MB @  12.8MB/s  0.9s\npkgs/main/linux-64   ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m   6.2MB /   6.3MB @   5.9MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64                                   6.3MB @   5.9MB/s  0.8s\n[+] 1.2s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  29.5MB /  36.8MB @  25.6MB/s  1.2s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  14.3MB /  16.0MB @  12.3MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  29.5MB /  36.8MB @  25.6MB/s  1.3s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  14.3MB /  16.0MB @  12.3MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  16.0MB @  13.3MB/s  1.2s\n[+] 1.4s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m  31.5MB /  36.8MB @  23.2MB/s  1.4s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  35.4MB /  36.8MB @  25.2MB/s  1.5s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  35.4MB /  36.8MB @  25.2MB/s  1.6s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.7s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  35.4MB /  36.8MB @  25.2MB/s  1.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.8s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  35.4MB /  36.8MB @  25.2MB/s  1.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                36.8MB @  25.9MB/s  1.8s\n\u001b[?25h\nPinned packages:\n  - python 3.10.*\n\n\nTransaction\n\n  Prefix: /opt/conda\n\n  Updating specs:\n\n   - aiohttp\n\n\n  Package    Version  Build            Channel           Size\n───────────────────────────────────────────────────────────────\n  Reinstall:\n───────────────────────────────────────────────────────────────\n\n  \u001b[32mo aiohttp\u001b[0m    3.9.1  py310h2372a71_0  conda-forge\u001b[32m     Cached\u001b[0m\n\n  Summary:\n\n  Reinstall: 1 packages\n\n  Total download: 0 B\n\n───────────────────────────────────────────────────────────────\n\n\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\nDownloading and Extracting Packages:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting xformers<0.0.26\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers<0.0.26) (1.26.4)\nCollecting torch==2.2.2 (from xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.2%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->xformers<0.0.26) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->xformers<0.0.26)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.2->xformers<0.0.26) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2->xformers<0.0.26) (1.3.0)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 torch-2.2.2+cu121 triton-2.2.0 xformers-0.0.25.post1\nCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-_l77dinf/unsloth_dd9fe58418da40bd956f2781b1df77b1\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-_l77dinf/unsloth_dd9fe58418da40bd956f2781b1df77b1\n  Resolved https://github.com/unslothai/unsloth.git to commit e4c8ceacb3fca634f78e662873a01c37678fcb3e\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2+cu121)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (21.3)\nCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.8.6-py3-none-any.whl.metadata (8.4 kB)\nCollecting transformers>=4.43.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m563.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\nRequirement already satisfied: sentencepiece>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\nRequirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.42.0)\nRequirement already satisfied: accelerate>=0.26.1 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.30.1)\nCollecting trl<0.9.0,>=0.7.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting peft!=0.11.0,>=0.7.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\nCollecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\nCollecting docstring-parser>=0.16 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[kaggle-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.6-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=138047 sha256=34f3e077d1443775e790fa2d2bfa575a462d1b75afe676639fe10b98fc9419c2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_8qjp0ag/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth, shtab, hf-transfer, docstring-parser, tyro, transformers, bitsandbytes, trl, peft\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.43.3 docstring-parser-0.16 hf-transfer-0.1.8 peft-0.12.0 shtab-1.7.1 transformers-4.44.0 trl-0.8.6 tyro-0.8.6 unsloth-2024.8\nCollecting datasets==2.16.0\n  Downloading datasets-2.16.0-py3-none-any.whl.metadata (20 kB)\nCollecting fsspec==2023.10.0\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nCollecting gcsfs==2023.10.0\n  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.6)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.0)\n  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0) (6.0.1)\nRequirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (5.1.1)\nRequirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (1.2.0)\nRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs==2023.10.0) (1.44.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0) (4.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs==2023.10.0) (4.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0) (2024.2.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs==2023.10.0) (1.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (1.16.0)\nRequirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.11.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.4.1)\nRequirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (2.7.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs==2023.10.0) (3.20.3)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0) (2023.4)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage->gcsfs==2023.10.0) (1.62.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->gcsfs==2023.10.0) (1.5.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==2023.10.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==2023.10.0) (3.2.2)\nDownloading datasets-2.16.0-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gcsfs-2023.10.0-py2.py3-none-any.whl (33 kB)\nDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, dill, multiprocess, datasets, gcsfs\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.3.1\n    Uninstalling fsspec-2024.3.1:\n      Successfully uninstalled fsspec-2024.3.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n  Attempting uninstall: gcsfs\n    Found existing installation: gcsfs 2024.3.1\n    Uninstalling gcsfs-2024.3.1:\n      Successfully uninstalled gcsfs-2024.3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\ns3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 gcsfs-2023.10.0 multiprocess-0.70.15\nCollecting deepspeed\n  Downloading deepspeed-0.14.4.tar.gz (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.26.4)\nRequirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.495.46)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.5.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed) (2.2.2+cu121)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed) (4.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.1.105)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\nDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.4-py3-none-any.whl size=1445506 sha256=bc033e7681002b9b7eef407f4f68b06b3466e8eebd7b3e882cda036aaeda0571\n  Stored in directory: /root/.cache/pip/wheels/8e/bc/a3/608e90bbb301848b78fd75d24d6d43ba3074de968fc0e397ac\nSuccessfully built deepspeed\nInstalling collected packages: hjson, deepspeed\nSuccessfully installed deepspeed-0.14.4 hjson-3.1.0\nCPU times: user 6.23 s, sys: 1.33 s, total: 7.56 s\nWall time: 5min 59s\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n    \"unsloth/llama-2-7b-bnb-4bit\",\n    \"unsloth/llama-2-13b-bnb-4bit\",\n    \"unsloth/codellama-34b-bnb-4bit\",\n    \"unsloth/tinyllama-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\ndef init_model(model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\"):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n    )\n\n    ## LoRA \n    ## 超参\n    ## r: 8 or 16\n    ## lora_dropout 0.2 or 0\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n       #peft_type=\"ADALORA\",\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 8,\n        lora_dropout = 0, # Supports any, but = 0 is optimized\n        bias = \"none\",    # Supports any, but = \"none\" is optimized\n        use_gradient_checkpointing = \"unsloth\", # 4x longer contexts auto supported!\n        random_state = 3407,\n        use_rslora = False,  # We support rank stabilized LoRA\n        loftq_config = None, # And LoftQ\n    )\n    return model,tokenizer\n\ndef save_checkpoint(dir='lora_model'):\n    model.save_pretrained(dir)\n    # model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n\ndef load_checkpoint(dir='lora_model'):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = dir, # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    return model,tokenizer\n\nmode='load'## 'build'\nmodel_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\"\ncheckpoint_dir='/kaggle/input/llama-3-qg-checkpoint-1/best_lora_model' ## 'best_lora_model'\nif mode=='build':\n    model,tokenizer=init_model(model_name)\nelse:\n    model,tokenizer=load_checkpoint(checkpoint_dir)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T15:52:24.933126Z","iopub.execute_input":"2024-08-10T15:52:24.933681Z","iopub.status.idle":"2024-08-10T15:52:36.146607Z","shell.execute_reply.started":"2024-08-10T15:52:24.933622Z","shell.execute_reply":"2024-08-10T15:52:36.145706Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.25.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## eval_model","metadata":{}},{"cell_type":"code","source":"!pip install pycocoevalcap\n!pip install bert_score","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T14:17:49.612871Z","iopub.execute_input":"2024-08-10T14:17:49.613560Z","iopub.status.idle":"2024-08-10T14:18:22.853370Z","shell.execute_reply.started":"2024-08-10T14:17:49.613527Z","shell.execute_reply":"2024-08-10T14:18:22.852167Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pycocoevalcap\n  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pycocotools>=2.0.2 (from pycocoevalcap)\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.16.0)\nDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.8\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.2+cu121)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.1)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.44.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2023.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.1.105)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n","output_type":"stream"}]},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom bert_score import BERTScorer\nimport numpy as np\n\nscorers = {\n        \"Bleu\": Bleu(4),\n        #\"Meteor\": Meteor(),\n        #\"Rouge\": Rouge(),\n    }\nimport bert_score\nbert_scorer = BERTScorer(lang=\"en\",model_type='roberta-large',rescale_with_baseline=True)\n# 测评问题的流畅性\ndef fluencyScore(preds_list, gold_list):\n    \n    gts = {}\n    res = {}\n    for i, (p, g) in enumerate(zip(preds_list, gold_list)):\n        gts[i] = [p]\n        res[i] = [g]\n    scores = {}\n    for name, scorer in scorers.items():\n        score, all_scores = scorer.compute_score(gts, res)\n        if isinstance(score, list):\n            for i, sc in enumerate(score, 1):\n                scores[name + str(i)] = sc\n        else:\n            scores[name] = score\n    return scores,all_scores[-1]\n\n# 测评语义相似度\ndef SemanticScore(preds_list, gold_list):\n    p,r,f1 = bert_scorer.score(preds_list, gold_list, verbose=True)\n    bert_score = np.mean(f1.tolist())\n    return bert_score\n\ndef getTotalScore(preds_list,gold_list):\n    bert_score = SemanticScore(preds_list,gold_list)\n    scores,all_scores = fluencyScore(preds_list,gold_list)\n    last_score = (bert_score/2+scores['Bleu4']/2)*100\n   # print(scores)\n    return {'TotalScore':last_score, \n            'BERTScore':bert_score,\n            'Bleu1':scores['Bleu1'],\n            'Bleu2':scores['Bleu2'],\n            'Bleu3':scores['Bleu3'],\n            'Bleu4':scores['Bleu4'],\n           },all_scores\n\n\n@torch.no_grad()\ndef generator(input):\n    inputs = tokenizer(\n    [\n        input\n    ],padding=True,truncation=True,max_length=max_seq_length, return_tensors = \"pt\").to(\"cuda\")\n    \n    outputs = model.generate(**inputs, \n                             max_new_tokens = 64, \n                             do_sample=False,\n                             #num_beams=3,\n                             use_cache = True,\n                            )\n    output=tokenizer.batch_decode(outputs,skip_special_tokens=True)\n    return output[0].split('### Response:\\n')[-1].replace('## Question:','').replace('\\n','')\n\nimport logging    \nlogging.getLogger().setLevel(logging.ERROR)\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR) \n\nbest_score=0\n## 评估模型\ndef eval_model(epoch=1,testing_batch=-1):\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    model.eval()\n    preds_list = []\n    real_list=[]\n    if testing_batch<=0:\n        testing_batch=len(dev_dataset)\n    with tqdm(total=testing_batch, desc=f'Validation Epoch {epoch}', unit='batch') as pbar:\n        for i in range(testing_batch):\n            if i>testing_batch:break\n            with torch.no_grad():\n                d=dev_dataset[i]\n                input=d['text']\n                question=d['real_output']\n                result=generator(input)\n                preds_list.append(result)\n                real_list.append(question)\n                \n            pbar.update(1)\n    model.train()\n    scores,all_score = getTotalScore(preds_list, real_list)\n    for i in range(20):\n        print('pred: ',preds_list[i])\n        print(\"true: \",real_list[i])\n        print('bleu4:',all_score[i])\n        \n    global best_score\n    if scores['TotalScore']>best_score:\n        print(f\"Total score: {best_score} -> {scores['TotalScore'] }\")\n        best_score=scores['TotalScore']\n        print('saving best model!')\n        save_checkpoint('best_lora_model')\n        \n    print(f\"Scores: {scores}\")\n    return preds_list","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-10T14:18:22.855148Z","iopub.execute_input":"2024-08-10T14:18:22.855549Z","iopub.status.idle":"2024-08-10T14:18:32.141126Z","shell.execute_reply.started":"2024-08-10T14:18:22.855509Z","shell.execute_reply":"2024-08-10T14:18:32.140208Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080e3169e03d429b966274ece4781f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8c89089cd34ad99c0dccf3f3010b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00777f1594c44582bc333144c77562f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443e75937c414105ad732dd48581faa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55663d1718b4758bb30e4e0698ab492"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8a1a1557eac4fe0a671cc633d457ca8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 训练","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom trl import DataCollatorForCompletionOnlyLM\n\nresponse_template = \"\\n ### Response:\"\nresponse_template_with_context = \" ### Response:\\n\"# We added context here: \"\\n\". This is enough for this tokenizer\nresponse_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n\ndata_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\ndef get_SFTTrainer(dataset,max_steps=60,lr=1e-4):\n    model.train()\n    trainer = SFTTrainer(\n        model = model,\n        tokenizer = tokenizer,\n        train_dataset = dataset,\n        dataset_text_field = \"text\",\n        max_seq_length = max_seq_length,\n        dataset_num_proc = 2,\n        data_collator=data_collator,\n        packing = False, # Can make training 5x faster for short sequences.\n        args = TrainingArguments(\n            per_device_train_batch_size = 2,\n            gradient_accumulation_steps = 8,\n            warmup_steps = 5,\n            max_steps = max_steps,\n            learning_rate = lr,\n            fp16 = not torch.cuda.is_bf16_supported(),\n            bf16 = torch.cuda.is_bf16_supported(),\n            logging_steps = 1,\n            optim = \"adamw_8bit\",\n            weight_decay = 0.01,\n            lr_scheduler_type = \"linear\",\n            seed = 3407,\n            output_dir = \"outputs\",\n            report_to = \"none\",\n            prediction_loss_only=True,\n            disable_tqdm=False,\n        ),\n        #deepspeed=ds_json\n    )\n    return trainer\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T14:18:32.146348Z","iopub.execute_input":"2024-08-10T14:18:32.146700Z","iopub.status.idle":"2024-08-10T14:18:32.157881Z","shell.execute_reply.started":"2024-08-10T14:18:32.146661Z","shell.execute_reply":"2024-08-10T14:18:32.156996Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 主要超参\n### 1.数据配比，质量较差的数据保留多少？\n##### 暂定不保留，清洗后训练数据剩余3.7w条\n### 2.指令如何设置，加不加思维链？如何加比较好？\n##### 待测\n### 3.lora的r设置为多少？dropout设置为多少？\n##### 单一任务r设置过大容易过拟合，r=8似乎效果不错\n##### dropout设置为0.2\n##### r=16,dropout=0 效果似乎不佳。\n### ~4.先进行指令微调是否有利于模型理解指令~\n##### LoRA不适合多阶段微调\n### 5.学习率如何设置？\n##### 暂定最大1e-4,并指数衰减到1e-5。2e-4时损失波动较大。\n### ~6.最大长度和pg_num如何设置~\n##### 如果文本被截断对模型有很大影响，直接设置最大长度为4096，pg_num=10","metadata":{}},{"cell_type":"code","source":"## 下游任务微调\n## 数据分桶，避免在同样数据上重复训练，影响模型性能\nnum_buckets=30 ## 数据桶数量\nbegin_epoch=12  ## 从断开位置继续训练\nend_epoch=26   ## 不超过桶的数量\ntotal_epoch=10 ## 预计总共要训的epoch数\nsteps=50     ## 每次训多少个batch\neval_epoch=1   ## 验证频率\nmax_lr=2e-4\nmin_lr=2e-5\ndecay=0.9\n##每个epoch训练100个batch,800个样本\nfor i in range(begin_epoch,end_epoch):\n    lr=max(max_lr*(decay**i),min_lr)\n    FastLanguageModel.for_training(model)\n    epoch_dataset=train_dataset.shard(num_shards=num_buckets,index=i)\n    trainer=get_SFTTrainer(epoch_dataset,steps,lr)\n    print('lr:',lr)\n    print('Training!')\n    trainer_stats = trainer.train()\n    print('avg_loss:',trainer_stats.training_loss)\n    save_checkpoint()\n    if i%eval_epoch==0:\n        eval_model(i,50)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 在验证集上测试","metadata":{}},{"cell_type":"code","source":"pred_list=eval_model(0,20)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T16:15:19.516790Z","iopub.execute_input":"2024-08-10T16:15:19.517860Z","iopub.status.idle":"2024-08-10T16:16:24.682218Z","shell.execute_reply.started":"2024-08-10T16:15:19.517812Z","shell.execute_reply":"2024-08-10T16:16:24.681141Z"},"scrolled":true,"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Validation Epoch 0: 100%|██████████| 20/20 [01:04<00:00,  3.24s/batch]","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63495674821b4a1abaa34955b3f045a3"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ee2db99042467aa251ca8722cc3129"}},"metadata":{}},{"name":"stdout","text":"done in 0.22 seconds, 91.64 sentences/sec\n{'testlen': 280, 'reflen': 274, 'guess': [280, 260, 240, 220], 'correct': [134, 64, 34, 17]}\nratio: 1.0218978102152485\npred:  Are Plantago and Trichosanthes both plant life?\ntrue:  Are Trichosanthes and Plantago both forms of plant life?\nbleu4: 6.936319082866856e-09\npred:  What season of the Indian reality TV series did Lopamudra Raut participate in?\ntrue:  In which season of the Indian reality TV show \"Big Boss\" did the model Lopamundra Raut participate?\nbleu4: 0.302770291955685\npred:  Which incumbent Republican U.S. Senator won re-election to his first full term in the 2012 United States Senate election in Nevada?\ntrue:  The 2012 United States Senate election in Nevada concluded with a close victory for which current Republican incumbent?\nbleu4: 0.23278057780280073\npred:  When did the character Moe Szyslak first appear on the Fox network in the United States?\ntrue:  When was the Simpson's episode broadcasted that introduced the character Morris \"Moe\" Szyslak?\nbleu4: 3.10215771223973e-09\npred:  Are Gene and Shipping News both bands?\ntrue:  Are Shipping News and Gene both rock bands?\nbleu4: 8.034284188172034e-09\npred:  Which author wrote more science fiction novels, Samuel R. Delany or Harlan Coben?\ntrue:  Who is the science fiction writer, Samuel R. Delany or Harlan Coben? \nbleu4: 0.4317280872519968\npred:  Who was the founder of the brewery that is now the second largest brewery in the United States?\ntrue:  Who was the initial female founder of the brewery that was acquired in 2006 by the biggest brewery in the state of Maryland?\nbleu4: 0.25924945759774676\npred:  What type of philosophies does the Structure of Liberty emphasize?\ntrue:  What type of ideologies is The Structure of Liberty about?\nbleu4: 5.555238066802865e-05\npred:  In what state is the Baltimore County, Maryland, located?\ntrue:  What characteristics do Lutherville and Baltimore County share? \nbleu4: 7.751502103232947e-13\npred:  Where is the company that Carolus Nolet is chairman of located?\ntrue:  Carolus Nolet is the chairman of a company from which city in the Netherlands?\nbleu4: 3.2857020445327865e-05\npred:  Which analyst has done play-by-play for the 2016–17 Kansas Jayhawks women's basketball team?\ntrue:  Who serves as an analyst for a TV network for the Kansas Jayhawks women's basketball team?\nbleu4: 0.27225894228808106\npred:  Which company is based in Biel/Bienne, Switzerland, Omega SA or EHC Biel?\ntrue:  What is the name of the company operated by Joseph Reiser located in Biel/Bienne, Switzerland?\nbleu4: 3.324137843193063e-09\npred:  What is the name of the province in the autonomous island region of Sicily in Italy that was established by Greek colonists arriving from Corinth in the eighth century B.C.?\ntrue:  What Province in Sicily, established in the eighth century B.C., was the birthplace of the renowned historical mathematician Archimedes?\nbleu4: 0.09829654836205383\npred:  What cartoon character created by Seymour Reit and Joe Oriolo did Sherri Stoner work on?\ntrue:  Which movie, based on a cartoon character by Seymour Reit and Joe Oriolo, did Deanna Oliver and Sherri Stoner work on?\nbleu4: 0.31642571775070466\npred:  Are Odyssey and Werther both novels?\ntrue:  Are Werther and The Odyssey both musicals? \nbleu4: 1.5619699681635397e-12\npred:  Gregor Fisher had a role in which Christmas-themed romantic comedy film?\ntrue:  What 2003 Christmas-themed romantic comedy did Gregor Fisher participate in?\nbleu4: 3.758250573156499e-05\npred:  Where is the college that Dexter Jackson played for located?\ntrue:  Where is the historically black college where Dexter Jackson played college football located?\nbleu4: 4.3945627746136275e-05\npred:  Where is the holding company for the regional airline that operates as American Eagle, Delta Connection, and United Express headquartered?\ntrue:  The Republic Airways Holdings company is located in the same place as which of its regional airline subsidiary's main offices?\nbleu4: 2.680165156076365e-09\npred:  In which part of England is the West Derby Hundred located?\ntrue:  Is Liverpool located in northern or southern England?\nbleu4: 1.0735252128116342e-16\npred:  In what year did the Nobel Prize in Physics go to a German-born theoretical physicist who developed the theory of relativity?\ntrue:  The physicist that noticed a flaw in Newton's classical theory of gravity received a Nobel Prize in what year?\nbleu4: 2.0285646317188276e-05\nScores: {'TotalScore': 36.91766849950473, 'BERTScore': 0.5488522298634052, 'Bleu1': 0.4785714285697194, 'Bleu2': 0.34322324775767155, 'Bleu3': 0.25554870791622464, 'Bleu4': 0.18950114012668942}\n","output_type":"stream"}]},{"cell_type":"code","source":"## 测试截断\ndef test_trunction(max_seq_len):\n    from tqdm import tqdm\n    c=0\n    for i in tqdm(range(2000)):\n        tokens=len(tokenizer(train_dataset[i]['text'])['input_ids'])\n        if tokens>max_seq_len:\n            c+=1\n    print(c)\n    print('被截断数据的比例:',c/2000)\n    \ntest_trunction(4096)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T15:37:25.346820Z","iopub.status.idle":"2024-08-10T15:37:25.347154Z","shell.execute_reply.started":"2024-08-10T15:37:25.346991Z","shell.execute_reply":"2024-08-10T15:37:25.347006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## zero-shot\n- TotalScore:-15.853062438587193\n- BERTScore: -0.3292037755716592\n- Bleu4: 0.012142526799915364\n\n## sft-training\n- TotalScore:14.13900637621369\n- BERTScore:0.2392001298184578\n- Bleu4:0.04357999770581597\n\n## 800examples\n- TotalScore:17.026505696049146\n- BERTScore:0.26740919244941325\n- Bleu4:0.07312092147156968\n- loss:1.04\n","metadata":{}},{"cell_type":"markdown","source":"## 在测试集上测试","metadata":{}},{"cell_type":"code","source":"test_data_path='/kaggle/input/scnu-ai-challenge-dataset-with-sorted-pred-facts/test.json'\nwith open(args.test_data_path, 'r', encoding='utf-8') as json_file:\n    data = json.load(json_file)\ntest_id_list = [item['_id'] for item in data]\n\n# 测试\ngenerated_questions = []\ngenerated_questions_dict = []\nFastLanguageModel.for_inference(model\nwith tqdm(total=len(test_dataset), desc=f'Test epoch {1}/{1}', unit='batch') as pbar:\n    for i in range(len(test_dataset)):\n        d=test_dataset[i]\n        with torch.no_grad():\n            input=d['text']\n            result=generator(input)\n            generated_questions.append(result)\n \n        pbar.update(1)\n\nfor _,item in enumerate(generated_questions):\n    generated_questions_dict.append({'_id':test_id_list[_],'question':item})\n\nwith open('output.json', 'w', encoding='utf-8') as json_file:\n        json.dump(generated_questions_dict, json_file, ensure_ascii=False, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T15:37:25.348438Z","iopub.status.idle":"2024-08-10T15:37:25.348778Z","shell.execute_reply.started":"2024-08-10T15:37:25.348611Z","shell.execute_reply":"2024-08-10T15:37:25.348625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### warning:\n#### llama用的tokenizer对上下文敏感（包括空格和换行）","metadata":{}},{"cell_type":"code","source":"def print_tokens_with_ids(txt):\n    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n    print(list(zip(tokens, token_ids)))\n\nprompt =train_dataset[0]['text']\nprint_tokens_with_ids(prompt[-500:])  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n\nresponse_template = \" ### Response:\\n\"\nprint_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T16:26:24.195994Z","iopub.execute_input":"2024-08-10T16:26:24.196472Z","iopub.status.idle":"2024-08-10T16:26:24.206729Z","shell.execute_reply.started":"2024-08-10T16:26:24.196432Z","shell.execute_reply":"2024-08-10T16:26:24.205865Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[('re', 265), ('Ġlimited', 7347), ('Ġto', 311), ('Ġthose', 1884), ('Ġcommon', 4279), ('Ġamong', 4315), ('Ġmany', 1690), ('Ġbreeds', 58245), ('Ġof', 315), ('Ġdog', 5679), (',', 11), ('Ġalthough', 8051), ('Ġthey', 814), ('Ġare', 527), ('Ġaffected', 11754), ('Ġmore', 810), ('Ġthan', 1109), ('Ġaverage', 5578), ('Ġby', 555), ('Ġhip', 18638), ('Ġdys', 22709), ('pl', 501), ('asia', 36259), ('Ġand', 323), ('Ġsome', 1063), ('Ġeye', 8071), ('Ġconditions', 4787), ('.', 13), ('Ġ', 220), ('ĠThey', 2435), ('Ġare', 527), ('Ġa', 264), ('Ġworking', 3318), ('Ġdog', 5679), (',', 11), ('Ġbred', 55187), ('Ġfor', 369), ('Ġhunting', 23330), (',', 11), ('Ġand', 323), ('Ġwhile', 1418), ('Ġnot', 539), ('Ġas', 439), ('Ġrare', 9024), ('Ġas', 439), ('Ġsome', 1063), ('Ġvarieties', 36680), ('Ġof', 315), ('Ġspan', 9575), ('iel', 13327), (',', 11), ('Ġthey', 814), ('Ġare', 527), ('Ġr', 436), ('arer', 61570), ('Ġthan', 1109), ('Ġthe', 279), ('Ġmore', 810), ('Ġwidely', 13882), ('Ġknown', 3967), ('ĠEnglish', 6498), ('ĠSpringer', 67915), ('ĠSpan', 12168), ('iel', 13327), ('Ġwith', 449), ('Ġwhich', 902), ('Ġthey', 814), ('Ġare', 527), ('Ġsometimes', 7170), ('Ġconfused', 22568), ('.', 13), ('ĠĊĊĊĊ', 23535), ('Ġ###', 17010), ('ĠResponse', 6075), (':Ċ', 512), ('##', 567), ('ĠQuestion', 16225), (':', 25), ('ĠWhich', 16299), ('Ġof', 315), ('Ġthe', 279), ('Ġtwo', 1403), ('Ġdog', 5679), ('Ġbreeds', 58245), (',', 11), ('Ġthe', 279), ('ĠEnglish', 6498), ('ĠSpringer', 67915), ('ĠSpan', 12168), ('iel', 13327), ('Ġor', 477), ('Ġthe', 279), ('ĠLand', 11680), ('se', 325), ('er', 261), (',', 11), ('Ġrecognized', 15324), ('Ġby', 555), ('Ġall', 682), ('Ġthe', 279), ('Ġkenn', 75361), ('el', 301), ('Ġclubs', 19424), ('?Ċ', 5380), ('<|end_of_text|>', 128001)]\n[('Ġ###', 17010), ('ĠResponse', 6075), (':Ċ', 512)]\n","output_type":"stream"}]}]}